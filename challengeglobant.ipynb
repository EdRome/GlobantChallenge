{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "068a3c5a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-27T17:52:18.955409Z",
     "iopub.status.busy": "2024-09-27T17:52:18.954819Z",
     "iopub.status.idle": "2024-09-27T17:52:19.940347Z",
     "shell.execute_reply": "2024-09-27T17:52:19.939147Z"
    },
    "papermill": {
     "duration": 0.995688,
     "end_time": "2024-09-27T17:52:19.943343",
     "exception": false,
     "start_time": "2024-09-27T17:52:18.947655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eee23d",
   "metadata": {
    "papermill": {
     "duration": 0.004458,
     "end_time": "2024-09-27T17:52:19.952828",
     "exception": false,
     "start_time": "2024-09-27T17:52:19.948370",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Discovery\n",
    "\n",
    "Before starting I need to understand the file type of the dataset, by listing all file in the directory and getting all file extension I can easily know the file type, therefore dealing with them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "753ef7dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T17:52:19.966136Z",
     "iopub.status.busy": "2024-09-27T17:52:19.965567Z",
     "iopub.status.idle": "2024-09-27T17:52:20.231566Z",
     "shell.execute_reply": "2024-09-27T17:52:20.230368Z"
    },
    "papermill": {
     "duration": 0.276158,
     "end_time": "2024-09-27T17:52:20.234096",
     "exception": false,
     "start_time": "2024-09-27T17:52:19.957938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.xml'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - os.listdir list all file in the directory\n",
    "# - os.path.splittext divide the filename into filename and extension, it returns a tuple and \n",
    "# the second element is the extension\n",
    "# - set returns the unique file extensions.\n",
    "xml_list_files = os.listdir('/kaggle/input/nsf-research-awards-abstracts')\n",
    "set([os.path.splitext(file)[1] for file in xml_list_files])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede1521f",
   "metadata": {
    "papermill": {
     "duration": 0.004678,
     "end_time": "2024-09-27T17:52:20.243864",
     "exception": false,
     "start_time": "2024-09-27T17:52:20.239186",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Since all files are XML, I'll be using ElemntTree from python that parse the file into a tree format to process and analyze it correctly.\n",
    "\n",
    "The XML content starts at `rootTag` that nestes `Award`. It contains lot of information like the title, agency, award expiration date, award amount, abstract, and much more.\n",
    "\n",
    "Since all this information not necessarly to classify all awards by topic, I'll be focusing in only three fields:\n",
    "\n",
    "1. `AwardTitle`: This will serve as a summary on the abstract. *Hypothesis*: Provides context about the topic.\n",
    "2. `Division`: The Division name could provide more information of the topic. *Hypothesis*: Each division will focus on knowledge areas.\n",
    "3. `AbstractNarration`: The abstract of the award, this has the more useful information to clusterize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9a2fe37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T17:52:20.255679Z",
     "iopub.status.busy": "2024-09-27T17:52:20.255247Z",
     "iopub.status.idle": "2024-09-27T17:52:20.270210Z",
     "shell.execute_reply": "2024-09-27T17:52:20.268653Z"
    },
    "papermill": {
     "duration": 0.024237,
     "end_time": "2024-09-27T17:52:20.273006",
     "exception": false,
     "start_time": "2024-09-27T17:52:20.248769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AwardTitle\n",
      "AGENCY\n",
      "AwardEffectiveDate\n",
      "AwardExpirationDate\n",
      "AwardTotalIntnAmount\n",
      "AwardAmount\n",
      "AwardInstrument\n",
      "Organization\n",
      "ProgramOfficer\n",
      "AbstractNarration\n",
      "MinAmdLetterDate\n",
      "MaxAmdLetterDate\n",
      "ARRAAmount\n",
      "TRAN_TYPE\n",
      "CFDA_NUM\n",
      "NSF_PAR_USE_FLAG\n",
      "FUND_AGCY_CODE\n",
      "AWDG_AGCY_CODE\n",
      "AwardID\n",
      "Investigator\n",
      "Institution\n",
      "Performance_Institution\n",
      "ProgramElement\n",
      "ProgramReference\n",
      "ProgramReference\n",
      "ProgramReference\n",
      "Appropriation\n",
      "Fund\n",
      "FUND_OBLG\n",
      "POR\n"
     ]
    }
   ],
   "source": [
    "tree = ET.parse('/kaggle/input/nsf-research-awards-abstracts/2000009.xml')\n",
    "root = tree.getroot()\n",
    "award = root.find(\"Award\")\n",
    "for child in award:\n",
    "    print(child.tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc872ec",
   "metadata": {
    "papermill": {
     "duration": 0.004789,
     "end_time": "2024-09-27T17:52:20.283074",
     "exception": false,
     "start_time": "2024-09-27T17:52:20.278285",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The dataset contains more than 13 thousand documents. To optimize the consumptions I'll be using iterators. My first thought was to use recursive functions; however, python has a limitation in the maximum recursion depth.\n",
    "\n",
    "On the other hand, iterators are lazy evaluators, being helpful to read a lot of files only when needed.\n",
    "\n",
    "To be more memory efficient, first I'll split the `xml_list_files` into training and test set. After that, I'll create batches on training data, so the consumptions will be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f623658a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T17:52:20.295430Z",
     "iopub.status.busy": "2024-09-27T17:52:20.294802Z",
     "iopub.status.idle": "2024-09-27T17:52:20.300713Z",
     "shell.execute_reply": "2024-09-27T17:52:20.299569Z"
    },
    "papermill": {
     "duration": 0.015308,
     "end_time": "2024-09-27T17:52:20.303649",
     "exception": false,
     "start_time": "2024-09-27T17:52:20.288341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recursive function. Not used because of maximum recursion depth\n",
    "# def parsing_docs(doc_list):\n",
    "#     docs = []\n",
    "#     directory = '/kaggle/input/nsf-research-awards-abstracts/'\n",
    "    \n",
    "#     # recursive case\n",
    "#     if len(doc_list) > 0:\n",
    "#         doc = doc_list[0]\n",
    "#         full_dir = os.path.join(directory, doc)\n",
    "#         award_title = ET.parse(full_dir).find(\"./Award/AwardTitle\")\n",
    "#         organization = ET.parse(full_dir).find(\"./Award/Organization\")\n",
    "#         abstract_narration = ET.parse(full_dir).find(\"./Award/AbstractNarration\")\n",
    "#         docs.append((award_title, organization, abstract_narration))\n",
    "#         docs.extend(parsing_docs(doc_list[1:]))\n",
    "        \n",
    "#     return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d73f7e29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T17:52:20.316422Z",
     "iopub.status.busy": "2024-09-27T17:52:20.315980Z",
     "iopub.status.idle": "2024-09-27T17:52:21.926750Z",
     "shell.execute_reply": "2024-09-27T17:52:21.925456Z"
    },
    "papermill": {
     "duration": 1.620547,
     "end_time": "2024-09-27T17:52:21.929831",
     "exception": false,
     "start_time": "2024-09-27T17:52:20.309284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the list into train and test\n",
    "train_list, test_list = train_test_split(xml_list_files, test_size=0.33, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58327c93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T17:52:21.942456Z",
     "iopub.status.busy": "2024-09-27T17:52:21.941812Z",
     "iopub.status.idle": "2024-09-27T17:52:21.950662Z",
     "shell.execute_reply": "2024-09-27T17:52:21.949383Z"
    },
    "papermill": {
     "duration": 0.018146,
     "end_time": "2024-09-27T17:52:21.953410",
     "exception": false,
     "start_time": "2024-09-27T17:52:21.935264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CreateTrainingBatches:\n",
    "    \n",
    "    def __init__(self, train_set, num_batches=4):\n",
    "        self.__train_set = train_set\n",
    "        # Define the batch size and the number of batches\n",
    "        self.__num_batches = num_batches\n",
    "        self.__train_size = len(self.__train_set)\n",
    "        self.__batch_size = round(self.__train_size / self.__num_batches)\n",
    "        \n",
    "    def create_batches(self):\n",
    "        # Yield an iterator\n",
    "        for i in range(self.__num_batches):\n",
    "            yield self.__train_set[i*self.__batch_size:(i+1)*self.__batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd3afdb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T17:52:21.966066Z",
     "iopub.status.busy": "2024-09-27T17:52:21.965630Z",
     "iopub.status.idle": "2024-09-27T17:52:21.981993Z",
     "shell.execute_reply": "2024-09-27T17:52:21.980711Z"
    },
    "papermill": {
     "duration": 0.025841,
     "end_time": "2024-09-27T17:52:21.984642",
     "exception": false,
     "start_time": "2024-09-27T17:52:21.958801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class XMLProcessor:\n",
    "    \n",
    "    def __init__(self, batches: iter):\n",
    "        self.__batches = batches\n",
    "    \n",
    "    def filter_xml_element(self, file_path):\n",
    "        tags_of_interest = {'AwardTitle','Organization','AbstractNarration'}\n",
    "        context = ET.iterparse(file_path)\n",
    "        return filter(lambda xml: xml[1].tag in tags_of_interest, context)\n",
    "\n",
    "    def process_files(self, xml_file):\n",
    "        return map(lambda file: self.filter_xml_element(file), xml_file)\n",
    "\n",
    "    def prepare_files(self, lst):\n",
    "        xml_dir = '/kaggle/input/nsf-research-awards-abstracts'\n",
    "        return map(lambda file: os.path.join(xml_dir, file), lst)\n",
    "\n",
    "    def get_text_elements(self, xml):\n",
    "        if xml[1].tag == 'Organization':\n",
    "            # Search for Division tag inside Organization\n",
    "            division = [elem for elem in xml[1] if elem.tag == 'Division']\n",
    "            # Inside Division, search to LongName (the name of the division)\n",
    "            long_name = [elem for elem in division[0] if elem.tag == 'LongName']\n",
    "            return long_name[0].text\n",
    "        else:\n",
    "            return xml[1].text\n",
    "\n",
    "    def get_data_from_generator(self, data):\n",
    "        return [map(lambda x: self.get_text_elements(x), elem) for elem in data]\n",
    "\n",
    "    def convert_to_dataframe(self, text_data):\n",
    "        return pd.DataFrame(\n",
    "            data=[list(data) for data in text_data],\n",
    "            columns=['AwardTitle','Division','AbstractNarration']\n",
    "        )\n",
    "\n",
    "    def process(self):\n",
    "        try:\n",
    "            lst = next(self.__batches)\n",
    "\n",
    "            xml_files = self.prepare_files(lst)\n",
    "            xml_data = self.process_files(xml_files)\n",
    "            xml_text_data = self.get_data_from_generator(xml_data)\n",
    "            return self.convert_to_dataframe(xml_text_data)\n",
    "        except StopIteration:\n",
    "            print(\"Iterator is empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a33e6d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T17:52:21.996596Z",
     "iopub.status.busy": "2024-09-27T17:52:21.996194Z",
     "iopub.status.idle": "2024-09-27T17:52:32.586247Z",
     "shell.execute_reply": "2024-09-27T17:52:32.585172Z"
    },
    "papermill": {
     "duration": 10.599224,
     "end_time": "2024-09-27T17:52:32.588987",
     "exception": false,
     "start_time": "2024-09-27T17:52:21.989763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_gen = CreateTrainingBatches(train_list)\n",
    "it = batch_gen.create_batches()\n",
    "\n",
    "xml_pr = XMLProcessor(it)\n",
    "# This method will generate a new dataframe each time is called.\n",
    "# This way I can explore a sample of 2 thousand records and, when needed, call the next 2 thounsand until the iterator is empty\n",
    "# When the last one occurs, it will print a message.\n",
    "df1 = xml_pr.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7694a06b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T17:52:32.601134Z",
     "iopub.status.busy": "2024-09-27T17:52:32.600719Z",
     "iopub.status.idle": "2024-09-27T17:52:32.627184Z",
     "shell.execute_reply": "2024-09-27T17:52:32.625922Z"
    },
    "papermill": {
     "duration": 0.035701,
     "end_time": "2024-09-27T17:52:32.629794",
     "exception": false,
     "start_time": "2024-09-27T17:52:32.594093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AwardTitle</th>\n",
       "      <th>Division</th>\n",
       "      <th>AbstractNarration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Assessing Students' Integration of Knowledge f...</td>\n",
       "      <td>Division Of Undergraduate Education</td>\n",
       "      <td>This project aims to serve the national intere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Collaborative Research: Apparatus for Normaliz...</td>\n",
       "      <td>Division Of Physics</td>\n",
       "      <td>A central goal of nuclear and particle physics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SBIR Phase I:  Novel injectable long-acting lo...</td>\n",
       "      <td>Translational Impacts</td>\n",
       "      <td>The broader impact /commercial potential of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Collaborative Research: FoMR: Taming the Instr...</td>\n",
       "      <td>Division of Computing and Communication Founda...</td>\n",
       "      <td>Data centers are the power plants that drive t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RAPID: Data-driven Multiscale Integrative Mode...</td>\n",
       "      <td>Division Of Chemistry</td>\n",
       "      <td>Gregory Voth of the University of Chicago is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>Processes Underlying the Rise of Social Comple...</td>\n",
       "      <td>Division Of Behavioral and Cognitive Sci</td>\n",
       "      <td>The goal of this project is to contribute to g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>Millennials and Corporate Employment Practices</td>\n",
       "      <td>Divn Of Social and Economic Sciences</td>\n",
       "      <td>Millennials are now the largest generation in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2225</th>\n",
       "      <td>FRG: Collaborative Research: Matroids, Graphs,...</td>\n",
       "      <td>Division Of Mathematical Sciences</td>\n",
       "      <td>Recent advances in matroid and graph theory fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2226</th>\n",
       "      <td>RAPID:NSF-BSF: Analysis of the spreading patte...</td>\n",
       "      <td>Division Of Environmental Biology</td>\n",
       "      <td>The recent COVID-19 pandemic has created an ur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>Morphology-Controlled Carbon Molecular Sieve M...</td>\n",
       "      <td>Div Of Chem, Bioeng, Env, &amp; Transp Sys</td>\n",
       "      <td>Energy industries, including oil and gas facil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2228 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             AwardTitle  \\\n",
       "0     Assessing Students' Integration of Knowledge f...   \n",
       "1     Collaborative Research: Apparatus for Normaliz...   \n",
       "2     SBIR Phase I:  Novel injectable long-acting lo...   \n",
       "3     Collaborative Research: FoMR: Taming the Instr...   \n",
       "4     RAPID: Data-driven Multiscale Integrative Mode...   \n",
       "...                                                 ...   \n",
       "2223  Processes Underlying the Rise of Social Comple...   \n",
       "2224     Millennials and Corporate Employment Practices   \n",
       "2225  FRG: Collaborative Research: Matroids, Graphs,...   \n",
       "2226  RAPID:NSF-BSF: Analysis of the spreading patte...   \n",
       "2227  Morphology-Controlled Carbon Molecular Sieve M...   \n",
       "\n",
       "                                               Division  \\\n",
       "0                   Division Of Undergraduate Education   \n",
       "1                                   Division Of Physics   \n",
       "2                                 Translational Impacts   \n",
       "3     Division of Computing and Communication Founda...   \n",
       "4                                 Division Of Chemistry   \n",
       "...                                                 ...   \n",
       "2223           Division Of Behavioral and Cognitive Sci   \n",
       "2224               Divn Of Social and Economic Sciences   \n",
       "2225                  Division Of Mathematical Sciences   \n",
       "2226                  Division Of Environmental Biology   \n",
       "2227             Div Of Chem, Bioeng, Env, & Transp Sys   \n",
       "\n",
       "                                      AbstractNarration  \n",
       "0     This project aims to serve the national intere...  \n",
       "1     A central goal of nuclear and particle physics...  \n",
       "2     The broader impact /commercial potential of th...  \n",
       "3     Data centers are the power plants that drive t...  \n",
       "4     Gregory Voth of the University of Chicago is s...  \n",
       "...                                                 ...  \n",
       "2223  The goal of this project is to contribute to g...  \n",
       "2224  Millennials are now the largest generation in ...  \n",
       "2225  Recent advances in matroid and graph theory fu...  \n",
       "2226  The recent COVID-19 pandemic has created an ur...  \n",
       "2227  Energy industries, including oil and gas facil...  \n",
       "\n",
       "[2228 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5774128,
     "sourceId": 9490588,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30775,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17.469021,
   "end_time": "2024-09-27T17:52:33.258313",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-27T17:52:15.789292",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
